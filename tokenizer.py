
from nltk.tokenize.punkt import PunktWordTokenizer
import networkx as nx

class TextToken:

    def __init__(seltf, ngram):

        self.value = ngram

class ElementToken:  

    def __init__(self, name=None, attrib=None, children=None, text=None, doc=None, **kwargs):

        if doc is not None:

            name = doc.xpath('local-name()')
            attrib = doc.attrib
            children = list(doc)
            text = doc.text

        self.name = name
        self.attrib = attrib

        # Generate a string consisting of the element name and concatenated attributes (for comparison using the edit distance)
        # Note: the athtributes *must* be order by some arbitrary feature

        self.value = self.name
        for attrib_name, attrib_value in self.attrib.iteritems():
            
            self.value += '_@' + attrib_name + '="' + attrib_value + '"'

        self.children = children

        self.text = text

# The Fragment Entity passed to the Juxta visualization interface
class Fragment:

    def __init__(self, tokens):

        self.value

class Tokenizer:

    def __init__(self):

        pass

    # Construct the parse tree
    # Each element is a node distinct from the 
    @staticmethod
    def parse(node):

        # Initialize an undirected graph for the tree, setting the root node to the lxml node
        token_tree = nx.Graph()
        token_tree_root = ElementToken(doc=node)

        # For the text of the node, use the PunktWordTokenizer to tokenize the text
        # Ensure that each tokenized n-gram is linked to the lxml token for the tree:
        #    o
        # /     \
        # n-gram n-gram

        # Default to Punkt
        tokenizer = PunktWordTokenizer()

        print 'trace3'
        print node
        print node.text
        print token_tree_root.name
        print token_tree_root.text

        # text_tokens = map( tokenizer.tokenize, token_tree_root.text )
        text_tokens = tokenizer.tokenize( token_tree_root.text )


        # if len(text_tokens) > 0:
        if True:

            # text_token_edges = map(lambda token: (token_tree_root, TextToken(token)), text_tokens )
            text_token_edges = map(lambda token: (token_tree_root.value, token), text_tokens )

        token_tree.add_edges_from(text_token_edges)

        children = list(node)
        
        # If the lxml has no more nodes, return the tree
        if len(children) == 0:

            return token_tree

        # ...otherwise, merge the existing tree with the sub-trees generated by recursion
        sub_trees = map(Tokenizer.parse, children)

        for sub_tree in map(Tokenizer.parse, children):

            token_tree = nx.compose(token_tree, sub_tree)
        return token_tree
